/***************************************************************************
 *            nonlinear_programming.dox
 *
 *  Copyright  2009  Pieter Collins
 *
 ****************************************************************************/

/*
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Library General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 */


/*!

\file nonlinear_programming.dox
\brief Documentation on nonlinear programming


\page nonlinear_programming_page NonLinear Programming

\section nonlinear_optimisation Standard optimisation problem

Consider the nonlinear programming problem
\f[ \framebox{$\max f(x) \text{ s.t. } g_j(x)\leq 0,\ j=1,\ldots,m_c; \ h_j(x)\leq 0,\ j=1,\ldots,m_e; \underline{x}_i \leq x_i \leq \overline{x}_i . $} \f]
The standard Karush-Kuhn-Tucker conditions for optimality are
\f[ \framebox{ $\begin{gathered} \nabla f(x) - \sum_{j=1}^{m_c} u_j \nabla g_j(x) + \sum_{j=1}^{m_e} v_j \nabla h_j(x) = 0; \\ \ u_j g_j(x)=0; \\ h_j(x)=0; \\ u_j,-g_j(x) \geq 0 \end{gathered}$ } \f]



\section constraints Handling constraints

\subsection inequality_constraints Equality Constraints

Consider the problem
\f[ \max f(x) \text{ s.t } g(x) = 0 \f]
The Karush-Kuhn-Tucker optimality conditions are
\f[ \nabla f(x) - \lambda \nabla g(x) = 0; \quad g(x) = 0. \f]

The problem can be relaxed by adding the penalty function \f$-g(x)^2/2\mu\f$.
Then the optimality conditions are
\f[ \nabla f(x) - \frac{g(x)}{\mu} \nabla g(x) = 0\f]
Taking Lagrange multiplier \f$\lambda = g(x)/\mu\f$, we obtain
\f[ \framebox{$ \begin{gathered} \nabla f(x) -  \lambda \nabla g(x) = 0; \\ g(x) - \mu \lambda = 0 \end{gathered} $} \f]
which relaxes to \f$g(x)=0\f$ as \f$\mu\to0\f$.


\subsection inequality_constraints Inequality Constraints

Consider the problem
\f[ \max f(x) \mid g(x) \geq 0 \f]
With the penalty function \f$ - \mu \log g(x)\f$, we obtain a critical point when
\f[ \nabla f(x) - \frac{\mu}{g(x)} \nabla g(x) = 0\f]
Take Lagrange multiplier \f$\lambda = \mu/g(x)\f$ to obtain the Lagrangian equation \f$ \nabla f(x) - \lambda \nabla g(x) = 0\f$ with
\f$ \lambda g(x) = \mu\f$.
Taking \f$\mu\to 0\f$, we obtain the standard Karush-Kuhn-Tucker conditions
\f[ \framebox{$ \begin{gathered}  \nabla f(x) - \lambda \nabla g(x) = 0; \\ \lambda g(x) = 0; \\ \lambda,\ g(x) \geq 0. \end{gathered} $} \f]

\subsection bounded_constraints Bounded Constraints

Consider the problem
\f[ \max f(x) \text{ s.t. } |g(x)| \leq \delta \f]
We can handle the problem in two ways; either by considering the smooth constraint \f$\delta^2-g(x)^2\geq0\f$ or the two constraints \f$\delta + g(x) \geq 0\f$ and \f$\delta - g(x)\geq 0\f$.

In the former case, we obtain the conditions
\f[ \begin{gathered} \nabla f(x) + 2 \lambda g(x) \nabla g(x) = 0; \\ \lambda (\delta^2 - g(x)^2) = \mu. \end{gathered} \f]
Replacing the standard Lagrange multiplier \f$\lambda\f$ with \f$-2\lambda g(x)\f$, we obtain
\f[ \begin{gathered} \nabla f(x) - \lambda \nabla g(x) = 0; \\ -\lambda (\delta^2 - g(x)^2) / 2g(x) = \mu. \end{gathered} \f]
Rearranging the second formula gives
\f[ \framebox{$ \begin{gathered} \nabla f(x) - \lambda \nabla g(x) = 0; \\ \bigl(\delta^2-g(x)^2\bigr) \lambda + 2 g(x) \mu = 0 . \end{gathered} $} \f]
Differentiating gives
\f[ (\delta^2-g(x)^2) {\Delta\lambda} + (2 \mu - 2 g(x) \lambda) \nabla g(x) {\Delta x} = 0\f]
In order to solve for \f${\Delta x}\f$ in terms of \f${\Delta\lambda}\f$, we require
\f$ \mu -  g(x) \lambda \neq 0\f$.
Since the optimal \f$x^*,\lambda^*\f$ satisfy \f$(\delta^2-g(x^*)^2) \lambda^* + 2 g(x^*) \mu = 0\f$, we have \f$\lambda^* g(x^*) \geq 0\f$.
Hence we can impose the feasibility constraint \f[g(x)\lambda \geq -\mu\f]

In the former case, we obtain the conditions
\f[ \begin{gathered} \nabla f(x) - \lambda^+ \nabla g(x) + \lambda^- \nabla g(x) = 0; \\ \lambda^+ (\delta + g(x)) = \mu; \\ \lambda^- (\delta - g(x)) = \mu \end{gathered} \f]
Set \f$\lambda = \lambda^+ - \lambda^-\f$. Then we have
\f[ \lambda = \mu \biggl( \frac{1}{\delta+g(x)} - \frac{1}{\delta-g(x)}\biggr) = \frac{-2\mu g(x)}{\delta^2-g(x)^2} . \f]
Rearranging again gives
\f[ (\delta^2-g(x)^2) \lambda + 2 g(x) \mu = 0\f]
Note that \f$\lambda^+ + \lambda^- = 2\delta\mu/(\delta^2-g(x)^2) \geq \bigl|\lambda^+-\lambda^-\bigr| \f$

\subsection state_constraints State constraints

Setting \f$g(x)=x\f$, we obtain the state constraints
\f[ \nabla f(x) - \lambda = 0; \quad \lambda = \frac{-2x\mu}{\delta^2-x^2} \f]
Differentiating yields
\f[ {\Delta\lambda} = -2\mu \frac{\delta^2+x^2}{(\delta^2-x^2)^2} {\Delta x} \f]
which can be used to eliminate these Lagrange multipliers entirely.



\section nonlinear_optimisation_newton Solving the Karush conditions by Newton-like methods

In the linear case \f$f(y)=b^Ty\f$, \f$g(y) = A^T y -c \f$ we have \f$\nabla^2 f = \nabla^2 g_i = 0\f$. Take \f$x=x_0\f$, \f$y_0=0\f$.
Then the condition \f$\nabla f(y_0) - x_0\cdot\nabla g(y_0) = 0\f$ becomes \f$b^T - x^T A^T = 0\f$ so \f$Ax=b\f$, and then \f$f(y) = b^T y \leq c^T x\f$, the usual duality bounds.

The first form suggests a solution method. We try to find \f$x_0\geq 0\f$ and \f$y_0\f$ (which need not be feasible) such that \f$x_0\cdot g(x_0)\f$ is small and \f$\nabla f(y_0) - x_0\cdot\nabla g(y_0) = 0\f$, and then the error only depends on \f$h(x_0,[y])- h(x_0,y_0)\f$ where \f$h(x,y) = \nabla f(y) - x\cdot \nabla g(y)\f$ and all derivatives are taken with respect to \f$y\f$. We subdivide as necessary to reduce the width of \f$h\f$.

For nonlinear systems, using a Newton method results in solving the equation
\f[ \framebox{$\displaystyle
    \left(\begin{matrix} Z&0&X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)
      \left(\begin{matrix} \Delta x \\ \Delta y \\ \Delta z \end{matrix}\right)
    = \left(\begin{matrix} r_x \\ r_y \\ r_z \end{matrix}\right)
    = \left(\begin{matrix} x \circ z \\ \nabla f(y) - x\cdot\nabla g(y) \\ g(y)+z \end{matrix}\right)
 $} \f]
where \f$A=A(y)=\nabla g(y)\f$ and \f$H=H(x,y)=\nabla^2 f(y) - x\cdot \nabla^2 g(y)\f$ and \f$D=D(x,z)=\mathrm{diag}(z)^{-1}\mathrm{diag}(x)\f$, so
\f[ \framebox{$ \displaystyle\
        A_{ij}(y) = \frac{\partial g_j(y)}{\partial y_i}; \qquad
        H_{ik}(x,y) = \frac{\partial^2 f(y)}{\partial y_i\partial y_k} - \sum_{j} x_j\,\frac{\partial^2 g_j(y)}{\partial y_i\partial y_k}; \qquad
        D_{jj}(x,z) = \frac{x_j}{z_j} .
\ $} \f]
Consider the inverse of the matrix
\f[
    \left(\begin{matrix} Z&0&X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)
        =
    \left(\begin{matrix} Z&0&0\\0&I&0\\0&0&I\\ \end{matrix}\right)
    \left(\begin{matrix} I&0&Z^{-1}X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)

\f]
We can factorise the original matrix as
\f[
    \left(\begin{matrix} Z&0&X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)
        =
    \left(\begin{matrix} Z&0&0 \\ 0&I&0 \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ -A&I&AZ^{-1}X \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ 0&H-AZ^{-1}XA^T&0 \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ 0&I&0 \\ 0&A^T&I \end{matrix}\right)
    \left(\begin{matrix} I&0&Z^{-1}X \\ 0&I&0 \\ 0&0&I \end{matrix}\right)
\f]
Writing \f$D=Z^{-1}X\f$ and \f$S=H-AZ^{-1}XA^T=H-ADA^T\f$  gives inverse
\f[
    \left(\begin{matrix} Z&0&X\\-A&H&0\\0&A^T&I\\ \end{matrix}\right)^{-1}
        =
    \left(\begin{matrix} I&0&-D \\ 0&I&0 \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ 0&I&0 \\ 0&-A^T&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ 0&S^{-1}&0 \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} I&0&0 \\ A&I&-AD \\ 0&0&I \end{matrix}\right)
    \left(\begin{matrix} Z^{-1}&0&0 \\ 0&I&0 \\ 0&0&I \end{matrix}\right)
\f]
A closed form for the inverse is
\f[ \left(\begin{matrix}
        Z^{-1}+Z^{-1}XA^TS^{-1}AZ^{-1} & Z^{-1}XA^TS^{-1} & -Z^{-1}X-Z^{-1}XA^TS^{-1}AZ^{-1}X\\
        S^{-1}AZ^{-1} & S^{-1} & -S^{-1}AZ^{-1}X \\
        -A^TS^{-1}AZ^{-1} & -A^TS^{-1} & I+A^TS^{-1}AZ^{-1}X \\
    \end{matrix}\right) \f]


If we have upper and lower bounds on the constraints \f$c^l\leq g(y) \leq c^u\f$, then the matrices \f$A\f$ and \f$D\f$ and \f$H\f$ are replaced by
\f[  \widehat{A} = \left(\begin{matrix}A(y)&-A(y)\end{matrix}\right);  \quad
     \widehat{D} = \left(\begin{matrix}D(x^u,z^u)&0\\0&D(x^l,z^l)\end{matrix}\right); \quad
     \widehat{H} = H(x^u-x^l,y) .
\f]


\section nonlinear_optimisation_dual_bounds Bounds on the objective function.

Consider the nonlinear programming problem without equality constraints,
\f[ \max f(x) \text{ s.t. } g_j(x)\leq 0,\ j=1,\ldots,m;\ x\in[x] . \f]

Suppose \f$x_0\f$ is any point in \f$[x]\f$ (not necessarily feasible) and \f$x_0\geq0\f$.
Then
\f[ \begin{aligned}
    f(x) &= f(x_0) + \nabla f([x]) (x-x_0) \\
         &= f(x_0) + \nabla f(x_0) (x-x_0) + \bigl(\nabla f([x])-\nabla f(x_0)\bigr)(x-x_0) \\
         &= f(x_0) + \bigl(\nabla f(x_0) - u_0\cdot \nabla g(x_0)\bigr)(x-x_0) + u_0\cdot\nabla g(x_0)(x-x_0) + \bigl(\nabla f([x])-\nabla f(x_0)\bigr)(x-x_0)
\end{aligned}\f]
Now
\f[ g(x) = g(x_0) + \nabla g(x_0) (x-x_0) + \bigl(\nabla g([x])-\nabla g(x_0)\bigr) (x-x_0) \f]
which upon rearranging yields
\f[  \nabla g(x_0) (x-x_0) = g(x)-g(x_0) - \bigl(\nabla g([x])-\nabla g(x_0)\bigr) (x-x_0) \f]
Substituting into the equation for \f$f(x)\f$ yields
\f[ \begin{aligned}
    f(x) &= f(x_0) + u_0\cdot\bigl(g(x)-g(x_0)\bigr) + \bigl(\nabla f(x_0) - u_0\cdot\nabla g(x_0)\bigr)(x-x_0)
              + \bigl(\nabla f([x])-\nabla f(x_0)\bigr)(x-x_0) - \bigl(\nabla g([x])-\nabla g(x_0)\bigr) (x-x_0) \\
         &= f(x_0) + u_0\cdot\bigl(g(x)-g(x_0)\bigr) + \bigl(\nabla f(x_0) - u_0\cdot\nabla g(x_0)\bigr)(x-x_0)
              + \bigl( (\nabla f([x])-u_0\cdot\nabla g([x])) - (\nabla f(x_0)-u_0\cdot\nabla g(x_0)) \bigr) (x-x_0)
\end{aligned}\f]
Using the fact that \f$g(x)\leq 0\f$, we obtain
\f[ \framebox{$ f(x) \leq f(x_0) - u_0\cdot g(x_0) + \bigl(\nabla f(x_0) - u_0\cdot\nabla g(x_0)\bigr)(x-x_0)
              + \bigl( (\nabla f([x])-u_0\cdot\nabla g([x])) - (\nabla f(x_0)-u_0\cdot\nabla g(x_0)) \bigr) (x-x_0)$}
\f]
This equation can also be written as
\f[ \framebox{$ f(x) \leq f(x_0) - u_0\cdot g(x_0) + \bigl( \nabla f(x_0)-u_0\cdot\nabla g(x_0) \bigr) (x-x_0) + \bigl( \nabla^2 f([x])-u_0\cdot\nabla^2 g([x]) \bigr) (x-x_0)^2$}
\f]
or
\f[ \mbox{$ f(x) \leq f(x_0) - u_0\cdot g(x_0) + \bigl( \nabla f([x])-u_0\cdot\nabla g([x]) \bigr) (x-x_0)$}
\f]



\section nonlinear_feasibility Feasibility problems

Consider the feasibility problem
\f[ g_j(y) \leq 0,\ j=1,\ldots,n;\ y\in[y] . \f]
We can re-cast this as an optimisation problem by introducing an extra variable \f$t\f$ and taking
\f[ \max t  \text{ s.t. } g_j(y)+t\leq 0,\ j=1,\ldots,n;\ y\in [y] . \f]
The Karush-Kuhn-Tucker conditions for optimality are
\f[ \mbox{$ \sum_{j=1}^{n} x_j \nabla g_j(y) = 0; \ \sum_{j=1}^{n} x_j =1; \ g_j(y) + t + z_j = 0; \ x_jz_j=0;\ x_j,z_j\geq 0,\ j=1,\ldots,n$}\f]
The problem has no solution if there exists \f$x^*\f$ such that
\f[ x^*_j \cdot g_j([y]) > 0 . \f]

If lower and upper bounds are used, and the variable \f$y\f$ is bounded, we have
\f[ c^l_j \leq g_j(y) \leq c^u_j,\ j=1,\ldots,n; \qquad d^l_i \leq y_i \leq d^u_i;\ i=1,\ldots,m . \f]
Although we can always easily satisfy the constraints \f$d^l_i \leq y_i \leq d^u_i\f$ e.g. by taking \f$y=(d^u+d^l)/2\f$, we need to add slackness to these constraints to find a "most feasible" point.
We make the constraints on \f$g\f$ into equality constraints by setting
\f[ g_j(y) + t + z^u_j = c^u_j; \  -g_j(y) + t + z^l_j = -c^l_j; \ j=1,\ldots,n. \f]
Taking extended variable \f$\hat{y}=(y,t)\f$, and letting \f$e^T\f$ denote a row vector all of whose elements are \f$1\f$
 we have matrices
\f[ \widehat{A} = \left(\begin{matrix} A(y)&-A(y)&I&-I \\ e^T&e^T&e^T&e^T \end{matrix}\right); \qquad
    \widehat{H} = \left(\begin{matrix} H(x^{cu}-x^{cl},y)&0 \\ 0&0 \end{matrix}\right); \qquad
    \widehat{D} = \left(\begin{matrix} D(x^{cu},z^{cu})&0&0&0 \\ 0&D(x^{cl},z^{cl})&0&0 \\ 0&0&D(x^{bu},z^{bu})&0 \\ 0&0&0&D(x^{bl},z^{bl}) \end{matrix}\right) .
\f]
yielding
\f[ \widehat{A}\widehat{D}\widehat{A}^T =
    \left(\begin{matrix} A(D^{cu}+D^{cl})A^T+(D^{bu}+D^{bl})&A(D^{cu}-D^{cl})e+(D^{bu}-D^{bl})e\\e^T(D^{cu}-D^{cl})A^T+e^T(D^{bu}-D^{bl})&e^T(D^{cu}+D^{cl}+D^{bu}+D^{bl})e \end{matrix}\right) . \f]
Note that considering lower and upper bounds requires no extra matrix multiplications, and recasting the problem as an optimisation problem in \f$m+1\f$ variables involves a single extra matrix multiplication \f$A(D^u-D^l)e^T\f$ and one extra variable in the matrix inversion. In other words, the extra work is minimal.

\remark
If \f$c^u_j=\infty\f$, then \f$z^u_j=\infty\f$ so \f$x^u_j=0\f$ and \f$D^u_{jj}=0\f$, so the constraint drops out of consideration in the inverse matrix. However, when computing the residuals, we obtain, assuming \f$0<x^{cu}_j,z^{cu}_j<\infty\f$ that \f$rz^{cu}_j=\infty\f$, and this propagates through the computation of the step to give \f$\delta x=\delta y=\delta z=\infty\f$. If we take \f$0<x^{cu}_j\f$ and \f$z^{cu}_j=\infty\f$, then the residual \f$rz^{cu}_j\f$ is \f$\mathrm{nan}\f$ which again propagates through the computation.

When some of the constraints \f$c^u\f$ or \f$c^l\f$ are infinite, then use of floating-point arithmetic will give nan in the corresponding constraints. In principle, we need to consider half-unbounded constraints directly, rather than as box constraints. However, since the result is bounded, we can take the unbounded part of these constraints large enough that they are automatically satisfied.


\section nonlinear_feasibility_algorithm Algorithms for Nonlinear Feasibility Problems

In %Ariadne, we are interested in solving feasibility problems where the constraints gradually tighten. This means that a feasable point can be used as in initial seed for iterating a tighter subproblem, and bound on the feasibly set of \f$y\f$ can be used in a tighter subproblem, and a certificate of infeasibility means that no further subproblems need to be considered.

A possible algorithm for solving feasibility problems is as follows:
 - Use a non-rigorous iterative nonlinear programming solver (e.g. an interior point solver) to find \f$x_0,y_0\f$ giving a local solution to the related optimality problem.
 - If \f$y_0\f$ is feasible, repeat for the tighter subproblems, using computed \f$(x_0,y_0)\f$ as a seed for the iterative solver.
 - If \f$x_0\cdot g([y])>0\f$, the problem is infeasible.
 - Reduce \f$[y]\f$ using the constraint \f$x_0\cdot g([y])\leq 0\f$ and hull/box reduce, possibly using the expansion \f$x_0\cdot g([y]) = x_0\cdot g(y_0)+\nabla g([y])(y-y_0)\f$.
 - If \f$[y]\f$ is empty, the problem is infeasible.
 - Apply the Krawczyk contractor to the Karush-Kuhn-Tucker optimality conditions \f$  x\cdot \nabla g(y) = 0; \ \sum x_i =1; \ g(y) + t + z = 0; \ x_iz_i=0; \ x_i,z_i\geq 0,\ i=1,\ldots,n\f$.
 - Split domain in two and repeat. The splitting should be done to increase \em linearity of the \f$\nabla g\f$, since linear solvers are very effective. Note that we should repeat from the beginning in case the failure to prove disjointness was due to \f$(x_0,y_0)\f$ being a local maximum.

 - It may be worth contracting \f$[y]\f$ from time to time to improve the iteration speed.
 - The use of \f$x_0\cdot g([y])\leq 0\f$ rather than \f$g_i([y])\leq 0\f$ to perform the contraction is to focus attention on the possibly violated constraints.
*/